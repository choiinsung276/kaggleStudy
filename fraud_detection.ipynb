{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/creditcardfraud/creditcard.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\nfrom tensorflow.keras.optimizers import Adam","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/creditcardfraud/creditcard.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape # 284807 개 , 31특성 ","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(284807, 31)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # 데이터 모두 float타입 ","execution_count":7,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Class.unique() # 1은 사기 0은 사기 아님 데이터 모두 0,1로 잘되있음","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"array([0, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Class.value_counts() # 사기인경우 492번, 나머지 정상 ","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0    284315\n1       492\nName: Class, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nf = df[df.Class==0] # 정상데이터만 따로 df\nf = df[df.Class==1] #사기데이터만 따로 df","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nf = nf.sample(738) # 정상이 너무 많으므로 사기데이터의 1.5배 만큼 랜덤 샘플한다.","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = f.append(nf,ignore_index=True) # data에는 60% 사기아닌것과 40% 사기 데이터가 있음","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape\nX = data.drop(['Class'],axis=1) # 사기 결과만 빼서 X에\ny=data['Class'] # 사기 결과 열만 따로 ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y)\nX_train.shape,X_test.shape\n# stratify 인자는 사기 데이터가 한쪽에 솔려서 분배되는것을 방지 ","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"((984, 30), (246, 30))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardscaler 데이터의 최소, 최대 모를경우 사용\n# robustscaler 이상치를 포함하는 데이터를 표준화하는경우\n# minmax데이터의 최소, 최대 값을 알경우 normal 중에 \n# standardscaler 사용\n# 각 피처의 평균을 0 , 분산을 1로 변경 \nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train) # fit, transform 작업 적용\nX_test=scaler.transform(X_test) # 테스트데이터에 적용해서 정규화 작업해주는경우는 transform만","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv1D(32,2,activation='relu',input_shape=X_train[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64,2,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,activation='sigmoid'))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":24,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d (Conv1D)              (None, 29, 32)            96        \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 29, 32)            128       \n_________________________________________________________________\ndropout (Dropout)            (None, 29, 32)            0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 28, 64)            4160      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 28, 64)            256       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 28, 64)            0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1792)              0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                114752    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 119,457\nTrainable params: 119,265\nNon-trainable params: 192\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,epochs=20,validation_data=(X_test,y_test))","execution_count":26,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n31/31 [==============================] - 0s 15ms/step - loss: 0.7259 - accuracy: 0.6982 - val_loss: 0.5738 - val_accuracy: 0.8699\nEpoch 2/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.3400 - accuracy: 0.8638 - val_loss: 0.4981 - val_accuracy: 0.8171\nEpoch 3/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.3054 - accuracy: 0.8882 - val_loss: 0.4433 - val_accuracy: 0.8537\nEpoch 4/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.2635 - accuracy: 0.9116 - val_loss: 0.3981 - val_accuracy: 0.8862\nEpoch 5/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.2426 - accuracy: 0.9106 - val_loss: 0.3565 - val_accuracy: 0.9024\nEpoch 6/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.2615 - accuracy: 0.9207 - val_loss: 0.3236 - val_accuracy: 0.9065\nEpoch 7/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.2291 - accuracy: 0.9197 - val_loss: 0.2927 - val_accuracy: 0.9106\nEpoch 8/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.2278 - accuracy: 0.9217 - val_loss: 0.2697 - val_accuracy: 0.9187\nEpoch 9/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.2164 - accuracy: 0.9228 - val_loss: 0.2532 - val_accuracy: 0.9268\nEpoch 10/20\n31/31 [==============================] - 0s 9ms/step - loss: 0.2119 - accuracy: 0.9360 - val_loss: 0.2443 - val_accuracy: 0.9309\nEpoch 11/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1860 - accuracy: 0.9380 - val_loss: 0.2399 - val_accuracy: 0.9228\nEpoch 12/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1809 - accuracy: 0.9319 - val_loss: 0.2394 - val_accuracy: 0.9268\nEpoch 13/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9411 - val_loss: 0.2387 - val_accuracy: 0.9268\nEpoch 14/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1883 - accuracy: 0.9390 - val_loss: 0.2420 - val_accuracy: 0.9268\nEpoch 15/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 0.9390 - val_loss: 0.2409 - val_accuracy: 0.9268\nEpoch 16/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1844 - accuracy: 0.9370 - val_loss: 0.2445 - val_accuracy: 0.9268\nEpoch 17/20\n31/31 [==============================] - 0s 9ms/step - loss: 0.1697 - accuracy: 0.9522 - val_loss: 0.2500 - val_accuracy: 0.9268\nEpoch 18/20\n31/31 [==============================] - 0s 9ms/step - loss: 0.1762 - accuracy: 0.9390 - val_loss: 0.2575 - val_accuracy: 0.9268\nEpoch 19/20\n31/31 [==============================] - 0s 8ms/step - loss: 0.1764 - accuracy: 0.9390 - val_loss: 0.2606 - val_accuracy: 0.9268\nEpoch 20/20\n31/31 [==============================] - 0s 7ms/step - loss: 0.1589 - accuracy: 0.9431 - val_loss: 0.2632 - val_accuracy: 0.9268\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  plt.plot(epochRange,history.history['accuracy'])\n  plt.plot(epochRange,history.history['val_accuracy'])\n  plt.title('Model Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()\n\n  plt.plot(epochRange,history.history['loss'])\n  plt.plot(epochRange,history.history['val_loss'])\n  plt.title('Model Loss')\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotLearningCurve(history,20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}